{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es Programación Dinámica y cómo se relaciona con RL?\n",
    "   La programación Dinámica se puede describir como una técnica de optimización para resolver problemas complejos en donde estos se dividen en subproblemas más simples, para esto se utilizan problemas que puedan mediante una estructura que sea recursiva. En el contexto de RL la programación dinámica se aplica para resolver problemas de decisión secuencial, donde un agente aprende a tomar decisiones para maximizar la recompensa acumulada a lo largo del tiempo, los algoritmos de programación dinámica como la iteración de valor y la iteración de póliza son métodos que se utilizan para encontrar políticas optimas y valores de estado.\n",
    "\n",
    "2. Explique en sus propias palabras el algoritmo de Iteración de Póliza.\n",
    "   En nuestras palabras el algoritmo de póliza se puede describir como un algoritmo de programación dinámica para encontrar una política optima el cual consiste dn dos pasos principales que se repiten varias veces siendo esta la evaluación de póliza que estima el valor de cada estado bajo una política fija y la mejora de póliza que es donde se actualiza la política eligiendo para cada estado, la acción que maximiza el valor esperado de la recompensa acumulada.\n",
    "\n",
    "3. Explique en sus propias palabras el algoritmo de Iteración de Valor\n",
    "   En nuestras palabras el algoritmo de iteración de valor se puede describir como un algoritmo de programación dinámica que busca encontrar una política optima, en donde este lo que hace es una actualización del valor para cada estado, se actualiza su valor tomando la máxima recompensa esperada que se puede obtener, considerando todas las posibles acciones y transiciones\n",
    "\n",
    "4. En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, ¿por qué?\n",
    "   Se puede mencionar que en el laboratorio anterior el valor de la recompensa obtenidas se mantiene constante porque las recompensas asignadas para cada transición de estado-acción están definidas de manera fijo en el entorno del problema por lo que independientemente de cuantas veces se realice la acción especifica en un estado dado la recompensa inmediata será siempre la misma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defina los estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = {x for x in range(0, 9)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defina las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state = 8\n",
    "obstacles = [2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidades de transición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#       [ 0   1   2]\n",
    "#       [ 3   4   5]\n",
    "#       [ 6   7   8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 1: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 2: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 3: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 4: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 5: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 6: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 7: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 8: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}}\n"
     ]
    }
   ],
   "source": [
    "# Inicialización de las probabilidades de transición y recompensas\n",
    "P = {s: {a: np.zeros(len(S)) for a in A} for s in S}\n",
    "R = {s: {a: np.zeros(len(S)) for a in A} for s in S}\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0]['up'] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0]['down'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[0]['left'] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0]['right'] = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# Estado 1\n",
    "P[1]['up'] = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1]['down'] = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1]['left'] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1]['right'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "# Estado 2\n",
    "P[2]['up'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[2]['down'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[2]['left'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[2]['right'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "# Estado 3\n",
    "P[3]['up'] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3]['down'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[3]['left'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[3]['right'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "# Estado 4\n",
    "P[4]['up'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[4]['down'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[4]['left'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[4]['right'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "# Estado 5\n",
    "P[5]['up'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[5]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[5]['left'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[5]['right'] = [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "# Estado 6\n",
    "P[6]['up'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[6]['down'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[6]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[6]['right'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "# Estado 7\n",
    "P[7]['up'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[7]['down'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[7]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7]['right'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "# Estado 8\n",
    "P[8]['up'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[8]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[8]['left'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[8]['right'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(s, a):\n",
    "    if a == 'up':\n",
    "        return s - 3 if s >= 3 else s\n",
    "    elif a == 'down':\n",
    "        return s + 3 if s <= 5 else s\n",
    "    elif a == 'left':\n",
    "        return s - 1 if s % 3 != 0 else s\n",
    "    elif a == 'right':\n",
    "        return s + 1 if s % 3 != 2 else s\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in S:\n",
    "    for a in A:\n",
    "        s_prime = move(s, a)\n",
    "        if s_prime == goal_state:\n",
    "            R[s][a][s_prime] = 10\n",
    "        elif s_prime in obstacles:\n",
    "            R[s][a][s_prime] = -10\n",
    "        elif s != s_prime:\n",
    "            R[s][a][s_prime] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {s: random.choice(A) for s in S}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    V = np.zeros(len(S))\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.001\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v = V[s].copy()\n",
    "            V[s] = max([sum([P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime])\n",
    "                            for s_prime in S])\n",
    "                        for a in A])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < epsilon:\n",
    "            print(f'Converged after {count} iterations')\n",
    "            break\n",
    "    \n",
    "    policy = {}\n",
    "    for s in S:\n",
    "        policy[s] = A[np.argmax([sum([P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime])\n",
    "                                    for s_prime in S])\n",
    "                                for a in A])]\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 89 iterations\n",
      "Optimal Value Function:\n",
      "[70.18153585 62.16338227  0.         79.09153585  0.         99.99153585\n",
      " 88.99153585 99.99153585 99.99153585]\n",
      "Optimal Policy:\n",
      "{0: 'down', 1: 'left', 2: 'down', 3: 'down', 4: 'up', 5: 'down', 6: 'right', 7: 'right', 8: 'down'}\n"
     ]
    }
   ],
   "source": [
    "V, policy = value_iteration()\n",
    "print(\"Optimal Value Function:\")\n",
    "print(V)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
