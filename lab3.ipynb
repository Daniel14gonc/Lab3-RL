{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es Programación Dinámica y cómo se relaciona con RL?\n",
    "2. Explique en sus propias palabras el algoritmo de Iteración de Póliza.\n",
    "3. Explique en sus propias palabras el algoritmo de Iteración de Valor\n",
    "4. En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, ¿por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defina los estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = {x for x in range(0, 9)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defina las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state = 8\n",
    "obstacles = [2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidades de transición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#       [ 0   1   2]\n",
    "#       [ 3   4   5]\n",
    "#       [ 6   7   8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 1: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 2: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 3: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 4: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 5: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 6: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 7: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}, 8: {'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'left': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'right': array([0., 0., 0., 0., 0., 0., 0., 0., 0.])}}\n"
     ]
    }
   ],
   "source": [
    "# Inicialización de las probabilidades de transición y recompensas\n",
    "P = {s: {a: np.zeros(len(S)) for a in A} for s in S}\n",
    "R = {s: {a: np.zeros(len(S)) for a in A} for s in S}\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0]['up'] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0]['down'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[0]['left'] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[0]['right'] = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# Estado 1\n",
    "P[1]['up'] = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1]['down'] = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1]['left'] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[1]['right'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "# Estado 2\n",
    "P[2]['up'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[2]['down'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[2]['left'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[2]['right'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "# Estado 3\n",
    "P[3]['up'] = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "P[3]['down'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[3]['left'] = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "P[3]['right'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "# Estado 4\n",
    "P[4]['up'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[4]['down'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[4]['left'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[4]['right'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "# Estado 5\n",
    "P[5]['up'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[5]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[5]['left'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[5]['right'] = [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "# Estado 6\n",
    "P[6]['up'] = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "P[6]['down'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[6]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[6]['right'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "# Estado 7\n",
    "P[7]['up'] = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "P[7]['down'] = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "P[7]['left'] = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "P[7]['right'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "# Estado 8\n",
    "P[8]['up'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[8]['down'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[8]['left'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "P[8]['right'] = [0, 0, 0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(s, a):\n",
    "    if a == 'up':\n",
    "        return s - 3 if s >= 3 else s\n",
    "    elif a == 'down':\n",
    "        return s + 3 if s <= 5 else s\n",
    "    elif a == 'left':\n",
    "        return s - 1 if s % 3 != 0 else s\n",
    "    elif a == 'right':\n",
    "        return s + 1 if s % 3 != 2 else s\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in S:\n",
    "    for a in A:\n",
    "        s_prime = move(s, a)\n",
    "        if s_prime == goal_state:\n",
    "            R[s][a][s_prime] = 10\n",
    "        elif s_prime in obstacles:\n",
    "            R[s][a][s_prime] = -10\n",
    "        elif s != s_prime:\n",
    "            R[s][a][s_prime] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {s: random.choice(A) for s in S}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    V = np.zeros(len(S))\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.001\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v = V[s].copy()\n",
    "            V[s] = max([sum([P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime])\n",
    "                            for s_prime in S])\n",
    "                        for a in A])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < epsilon:\n",
    "            print(f'Converged after {count} iterations')\n",
    "            break\n",
    "    \n",
    "    policy = {}\n",
    "    for s in S:\n",
    "        policy[s] = A[np.argmax([sum([P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime])\n",
    "                                    for s_prime in S])\n",
    "                                for a in A])]\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 89 iterations\n",
      "Optimal Value Function:\n",
      "[70.18153585 62.16338227  0.         79.09153585  0.         99.99153585\n",
      " 88.99153585 99.99153585 99.99153585]\n",
      "Optimal Policy:\n",
      "{0: 'down', 1: 'left', 2: 'down', 3: 'down', 4: 'up', 5: 'down', 6: 'right', 7: 'right', 8: 'down'}\n"
     ]
    }
   ],
   "source": [
    "V, policy = value_iteration()\n",
    "print(\"Optimal Value Function:\")\n",
    "print(V)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
